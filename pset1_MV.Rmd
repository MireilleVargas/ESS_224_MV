---
title: "Pset 1"
author: "Mireille Vargas"
date: "`r format(Sys.Date(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(raster)
library(ncdf4)
library(rgdal)
library(tidyverse)
library(stringr)
library(ggplot2)
library(tigris)
library(leaflet)
library(mapview)
library(sf)
library(knitr)
```

Downloading the soil data
```{r}
soil<- read.csv("C:/Users/mireille/Documents/Data/pset1/Q1_soil_moisture.csv")
```

# Question 1 
Ms. Turo, a farm owner from Narnia, approaches you to help her choose the most ideal crop rotation scheme, for which she needs accurate soil moisture values for an entire year. You have assembled soil moisture from three datasets with independent error sources. These are provided in the Canvas file Q2_soil_moisture.csv in several columns, with the date in the first column. The units are cm3/cm3. You want to better understand your datasets in order to figure out what information to pass along to her. 

a.) (2 points) Assuming the first dataset (e.g. that in the second of four columns in the file) is the reference dataset, use triple collocation to estimate the additive and multiplicative biases of the other two datasets.

```{r}
#calculate the covariance
#Q23

Q23<- cov(soil$X2, soil$X3)

#Q13
Q13 <- cov(soil$X1, soil$X3)

#Q12

Q12<- cov(soil$X1, soil$X2)

#looking for B2 and B3
#B2 = Q23/Q13

B2 <- Q23/Q13

#B3 = Q23/Q12

B3 <- Q23/Q12

#looking for a2 and a3

#a2 = average x 2 - B2 average x1
a2 <- mean(soil$X2) - (B2 * mean(soil$X1))

#a3 = average x3 - B3 average x1
a3 <- mean(soil$X3) - (B3 * mean(soil$X1))

#answers
print("The additive biases of the two data sets are")
print(c(a2, a3))
print("The multiplicative biases of the two data sets are")
print(c(B2, B3))
```


b.) (1 point) What is the variance of the random error of each of the three data sets? 
```{r}
#variance is the square root sd
#random error RMSE: "square root of the variance of the residuals

#O1 = sqrt(Q11 - Q12)

#But then you want to square that so the "variance of the random error" woult then just be Q11 - Q12

#Variance of random error for d1

v1 <- cov(soil$X1, soil$X1) - cov(soil$X1, soil$X2)

#Variance of random error for d2
v2 <- cov(soil$X2, soil$X2) - cov(soil$X1, soil$X2)

#Variance of random error for d3
v3 <- cov(soil$X3, soil$X3) - cov(soil$X1, soil$X3)

#answers
print("The variance of the random error of each of the three data sets")
print(c(v1, v2, v3))
```


c.) (1 point) What is the correlation between each of the 3 datasets and the ‘true’ soil moisture?
```{r}
#for in situ and true signal
Q11 <- cov(soil$X1, soil$X1)

cor_in_situ <- sqrt((Q12 * Q13)/(Q11 * Q23))


#for second dataset
Q22 <- cov(soil$X2, soil$X2)
cor_x2 <- sign(Q13*Q23) * sqrt((Q12* Q23)/ (Q22 * Q13))

#for third dataset
Q33<- cov(soil$X3, soil$X3)
cor_x3 <- sign(Q12*Q23) * sqrt((Q13* Q23)/ (Q33 * Q12))

#answers
print("The correlation between each of the 3 datasets and the ‘true’ soil moisture")
print(c(cor_in_situ, cor_x2, cor_x3))
```

# Question 2
This problem considers several possible ways to use data from the GRACE satellite. In each case, please describe whether this use case is appropriate and if not, why not.

a.) (1 point)  Calculating the exact amount of water in the San Joaquin Valley Aquifer

This is not an appropriate case of the GRACE satellite because satellite data cannot give exact measurement. Furthermore, GRACE data should not be the only data one should base their results on for grabbing an exact-or close to exact- measurement.


b.) (1 point) Estimating ET in the Amazon river basin, by comparing GRACE changes in total water storage, rainfall from a source you trust, and measured basin runoff.

This is a suitable case of using GRACE because GRACE gives terrestrial water storage values which includes groundwater, surface waters, snow ice, and soil moisture. Using various images of GRACE data can give a sense of the changes in water storage. 

c.) (1 point) Your buddy at the Iowa Department of Agriculture and Land Stewardship has spent a lot of time making a map of different irrigation types (each with different approximate levels of water use) employed by farms across the state. It turns out the choice of irrigation type is driven by a variety of factors and does not have a particular geographic pattern. You wish to calculate whether the irrigation type has an influence on the rate of groundwater depletion using GRACE.

Data from GRACE satellite will not be appropriate because GRACE data has a coarse spatial resolution of 300-400 km and the state of Iowa is roughly a 145,000 square kilometers big. The spatial resolution is not fine enough to detect different irrigation type influences on the rate of groundwater depletion.


# Question 3
This questions asks you to download, process, and interpret GRACE data from the 3 possible solutions: JPL, GFZ, and CSR. These data can be downloaded from Github at https://github.com/conordoherty/ESS_224-CEE_260D/tree/master/hw1, or alternatively from the web at https://podaac.jpl.nasa.gov/GRACE(under “TELLUS GRACE Level-3 Monthly Land Water-Equivalent-Thickness Surface Mass Anomaly Release 6.0 version 03”). There is also some Python starter code for loading the data in a Google colab notebook at https://colab.research.google.com/drive/1wIYY8jvvLFxjINQHk5sZUmkRGfHSMdiL?authuser=1#scrollTo=EgNebQdkr7Wn

a.) (2 point) Extract the monthly change in water storage for each of the three GRACE models at the point -120.5, 37.5, a location near Hopeton, CA in the Central Valley (about midway between Modesto and Merced). Plot the individual solution estimates and the average of the three solutions on a single time series plot. Note that there are some gaps in the data record (explanation here: https://grace.jpl.nasa.gov/data/grace_months/). You’ll need to extract the day-of-year ranges from the filenames and convert them to dates to generate the time series.

```{r, warning=FALSE, message=FALSE}
#jpl data
jpl <- list.files(path = "C:/Users/mireille/Documents/GitHub/ESS_224-CEE_260D/hw1/jpl_grace", pattern = "*.nc", full.names = TRUE)

#gfz data
gfz <- list.files(path = "C:/Users/mireille/Documents/GitHub/ESS_224-CEE_260D/hw1/gfz_grace", pattern = "*.nc", full.names = TRUE)

#csr data
csr <- list.files(path = "C:/Users/mireille/Documents/GitHub/ESS_224-CEE_260D/hw1/csr_grace", pattern = "*.nc", full.names = TRUE)
```

Create a stack of rasters and rotate so that 0-360 is to -180 to 180.
```{r, warning=FALSE, message=FALSE, results='hide'}
#jpl data
jpl_ras <- lapply(jpl, raster::raster, lvar = 4, level = 1)

jpl_brick <- raster:: brick(jpl_ras) %>% 
  rotate()

#gfz data
gfz_ras <- lapply(gfz, raster::raster, lvar = 4, level = 1)

gfz_brick <- raster:: brick(gfz_ras) %>% 
  rotate()

#csr data
csr_ras <- lapply(csr, raster::raster, lvar = 4, level = 1)

csr_brick <- raster:: brick(csr_ras) %>% 
  rotate()
```

Extract the value at the given location.
```{r, warning=FALSE, message=FALSE}
#jpl data
jpl_vals <- raster:: extract(jpl_brick, matrix(c(-120.5, 37.5), ncol = 2), fun = mean, df = TRUE) 
jpl_vals <- jpl_vals %>% t() %>% as.data.frame()
jpl_vals <- jpl_vals[-1,] %>% 
  as.data.frame()

#gfz data
gfz_vals <- raster:: extract(gfz_brick, matrix(c(-120.5, 37.5), ncol = 2), fun = mean, df = TRUE) 
gfz_vals <- gfz_vals %>% t() %>% as.data.frame()
gfz_vals <- gfz_vals[-1,] %>% 
  as.data.frame()

#csr data
csr_vals <- raster:: extract(csr_brick, matrix(c(-120.5, 37.5), ncol = 2), fun = mean, df = TRUE) 
csr_vals <- csr_vals %>% t() %>% as.data.frame()
csr_vals <- csr_vals[-1,] %>% 
  as.data.frame()
```

Now grab the date column from the list of files and add to the dataframe.

```{r}
date_format <- function(data1, data2){
date_list<- c()
#data2$dates <- ""
for (i in 1:length(data1)){
  test_p1 <- str_sub(data1[i], start = 73, end = 79)
  test_year <- str_sub(test_p1, start = 1, end = 4) %>% as.numeric()
  test_days <- str_sub(test_p1, start = 5, end = 7) %>%  as.numeric()
  if (test_year %% 4 == 0){
    if (test_days < 31){
  month <- 01
  day <- test_days
} else if (test_days > 31 && test_days < 60){
  month <- 02
  day <- test_days - 31
} else if  (test_days > 60 && test_days < 91){
  month <- 03
  day <- test_days - 60
} else if  (test_days > 91 && test_days < 121){
  month <- 04
  day <- test_days - 91
} else if  (test_days > 121 && test_days < 152){
  month <- 05
  day <- test_days - 121
}  else if  (test_days > 152 && test_days < 182){
  month <- 06
  day <- test_days - 152
} else if  (test_days > 182 && test_days < 213){
  month <- 07
  day <- test_days - 182
} else if  (test_days > 213 && test_days < 244){
  month <- 08
  day <- test_days - 213
}  else if  (test_days > 244 && test_days < 274){
  month <- 09
  day <- test_days - 244
} else if  (test_days > 274 && test_days < 305){
  month <- 10
  day <- test_days - 274
} else if  (test_days > 305 && test_days < 335){
  month <- 11
  day <- test_days - 305
}else {
  month <- 12
  day <- test_days - 335
}
  }else{
    if (test_days < 31){
  month <- 01
  day <- test_days
} else if (test_days > 31 && test_days < 59){
  month <- 02
  day <- test_days - 31
} else if  (test_days > 59 && test_days < 90){
  month <- 03
  day <- test_days - 59
} else if  (test_days > 90 && test_days < 120){
  month <- 04
  day <- test_days - 90
} else if  (test_days > 120 && test_days < 151){
  month <- 05
  day <- test_days - 120
}  else if  (test_days > 151 && test_days < 181){
  month <- 06
  day <- test_days - 151
} else if  (test_days > 181 && test_days < 212){
  month <- 07
  day <- test_days - 181
} else if  (test_days > 212 && test_days < 243){
  month <- 08
  day <- test_days - 212
}  else if  (test_days > 243 && test_days < 273){
  month <- 09
  day <- test_days - 243
} else if  (test_days > 273 && test_days < 304){
  month <- 10
  day <- test_days - 273
} else if  (test_days > 304 && test_days < 334){
  month <- 11
  day <- test_days - 304
}else {
  month <- 12
  day <- test_days - 334
}
  }
 #day <- 0
 date <- paste(as.character(month), as.character(day), as.character(test_year), sep = "/")
 #date <- paste(as.character(month), as.character(test_year), sep = "/")
 date_list <- append(date_list, date)
  
}

data2$dates <- date_list
data2$dates <- as.Date(data2$dates, format = "%m/%d/%Y")
# data2$lwe_thickness <- data2$.
names(data2)[1] <- "lwe_thickness"
return(data2)
}

jpl_data <- date_format(jpl, jpl_vals)
gfz_data <- date_format(gfz, gfz_vals)
csr_data <- date_format(csr, csr_vals)
```


Grabbing the average lwe_thickness for each observation across the three different GRACE data.
```{r}
#mean_jpl <- mean(jpl_data$lwe_thickness)

avg_data <- data.frame(jpl_lwe = jpl_data$lwe_thickness,
                          gfz_lwe = gfz_data$lwe_thickness,
                          csr_lwe = csr_data$lwe_thickness,
                          dates = csr_data$dates
                          )

avg_list <- c()
for (i in 1:nrow(avg_data)){
  avg_i <- (avg_data[i,1] + avg_data[i,2] + avg_data[i,3])/3
  avg_list<- append(avg_list, avg_i)
}

avg_data$lwe_thickness <- avg_list

#modify avg data frame so it is like the others
avg_data$type <- "avg"
avg_data <- avg_data %>% 
  select(lwe_thickness, dates, type)
```


Now plot the data
```{r}
#create new column that puts the type that it is
jpl_data$type <- "jpl"
gfz_data$type <- "gfz"
csr_data$type <- "csr"

#combine the data to plot
all_data <- rbind(jpl_data, gfz_data, csr_data, avg_data)
```

```{r}
ggplot(data = all_data, aes(x = dates, y = lwe_thickness, group = type, color = type)) + geom_line() + labs(title = "GRACE data of Monthly Changes in LWE Thickness at (-120.5, 37.5)", y = "Date", x = "Monthly Changes in LWE thickness (m)") 

```
```{r}
ggplot(data = all_data %>% 
         filter(all_data$dates < "2003-01-01"), 
       aes(x = dates, y = lwe_thickness, group = type, color = type)) + geom_line()
```




b)(1 point) Interpret your plot in part a in the context of hydrological processes, climate, and water use in California. What trends do you observe? What might be causing the trends?

We see in this graph that total water storage decreases until mid fall and then total water storage increases again. We would expect to see less water during the summer and fall than in spring and winter. Over the years, total water storage has fluctuated, but since roughly 2013 there has been more lower peaks than in the years before. This is consistent with the California drought from December 2011 to March 2017, which was one of the most intense droughts in California history.



c)(1 point extra credit) Plot a map across the Continental US (or this approximate region) that shows the standard deviation of the three solution estimates at each grid cell for August 2016. What could be learned from a map like this?
```{r, warning=FALSE, message=FALSE, results='hide'}
#C:\Users\mireille\Documents\Data\pset1
aug <- list.files(path = "C:/Users/mireille/Documents/Data/pset1", pattern = "*.nc", full.names = TRUE)

aug_ras <- lapply(aug, raster::raster, lvar = 4, level = 1)

aug_brick <- raster:: brick(aug_ras) %>% 
  rotate() #%>% 
  #projectRaster(crs = projection)

#calculate standard deviation
aug_sd <- calc(aug_brick, fun = sd) 
```

```{r, warning=FALSE, message=FALSE, results='hide'}
#projection
#projection <- "+proj=utm +zone=10 +ellps=GRS80 +datum=NAD83 +units=ft +no_defs"

#grab states for US

states <- states(cb = TRUE) %>% 
  filter(NAME != "United States Virgin Islands",
         NAME != "Commonwealth of the Northern Mariana Islands",
          NAME != "Puerto Rico",
           NAME != "Guam",
         NAME != "American Samoa")

aug_mask <- mask(aug_sd, states)
#mapview(aug_mask)
```

```{r}
knitr::include_graphics("C:/Users/mireille/Documents/Data/pset1/Q3P3_pset1.png")
```


Standard deviation tells us the spread of the data and gives a measurement of how far each observed value is from the mean. A map like this can tell us areas that have more or less than the average total water storage for a given period of time.