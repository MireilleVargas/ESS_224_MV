---
title: "Pset1"
author: "Mireille Vargas"
date: "4/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(raster)
library(rgdal)
library(tidyverse)
library(ncdf4)
library(ggplot2)
library(mapview)
library(leaflet)
library(exactextractr)
library(sf)
library(RNetCDF)
library(parallel)
library(sp)
library(magrittr)
library(data.table)
```

```{r}
soil<- read.csv("C:/Users/mirei/OneDrive/Desktop/ESS224/Q1_soil_moisture.csv")
```

# Question 1 
Ms. Turo, a farm owner from Narnia, approaches you to help her choose the most ideal crop rotation scheme, for which she needs accurate soil moisture values for an entire year. You have assembled soil moisture from three datasets with independent error sources. These are provided in the Canvas file Q2_soil_moisture.csv in several columns, with the date in the first column. The units are cm3/cm3. You want to better understand your datasets in order to figure out what information to pass along to her. 

a.) (2 points) Assuming the first dataset (e.g. that in the second of four columns in the file) is the reference dataset, use triple collocation to estimate the additive and multiplicative biases of the other two datasets.
```{r}
#calculate the covariance
#Q23

Q23<- cov(soil$X2, soil$X3)

#Q13
Q13 <- cov(soil$X1, soil$X3)

#Q12

Q12<- cov(soil$X1, soil$X2)

#looking for B2 and B3
#B2 = Q23/Q13

B2 <- Q23/Q13

#B3 = Q23/Q12

B3 <- Q23/Q12

#looking for a2 and a3

#a2 = average x 2 - B2 average x1
a2 <- mean(soil$X2) - (B2 * mean(soil$X1))

#a3 = average x3 - B3 average x1
a3 <- mean(soil$X3) - (B3 * mean(soil$X1))

#answers
print(c(B2, B3, a2, a3))
```


b.) (1 point) What is the variance of the random error of each of the three data sets? 
```{r}
#variance is the square root sd
#random error RMSE: "square root of the variance of the residuals

#O1 = sqrt(Q11 - Q12)

#But then you want to square that so the "variance of the random error" woult then just be Q11 - Q12

#Variance of random error for d1

v1 <- cov(soil$X1, soil$X1) - cov(soil$X1, soil$X2)

#Variance of random error for d2
v2 <- cov(soil$X2, soil$X2) - cov(soil$X1, soil$X2)

#Variance of random error for d3
v3 <- cov(soil$X3, soil$X3) - cov(soil$X1, soil$X3)

#answers
print(c(v1, v2, v3))
```


c.) (1 point) What is the correlation between each of the 3 datasets and the ‘true’ soil moisture?
```{r}
#do a cor.test??
```

# Question 2
This problem considers several possible ways to use data from the GRACE satellite. In each case, please describe whether this use case is appropriate and if not, why not.

a.) (1 point)  Calculating the exact amount of water in the San Joaquin Valley Aquifer

b.) (1 point) Estimating ET in the Amazon river basin, by comparing GRACE changes in total water storage, rainfall from a source you trust, and measured basin runoff

c.) (1 point) Your buddy at the Iowa Department of Agriculture and Land Stewardship has spent a lot of time making a map of different irrigation types (each with different approximate levels of water use) employed by farms across the state. It turns out the choice of irrigation type is driven by a variety of factors and does not have a particular geographic pattern. You wish to calculate whether the irrigation type has an influence on the rate of groundwater depletion using GRACE

# Question 3

This questions asks you to download, process, and interpret GRACE data from the 3 possible solutions: JPL, GFZ, and CSR. These data can be downloaded from Github at https://github.com/conordoherty/ESS_224-CEE_260D/tree/master/hw1, or alternatively from the web at https://podaac.jpl.nasa.gov/GRACE(under “TELLUS GRACE Level-3 Monthly Land Water-Equivalent-Thickness Surface Mass Anomaly Release 6.0 version 03”). There is also some Python starter code for loading the data in a Google colab notebook at https://colab.research.google.com/drive/1wIYY8jvvLFxjINQHk5sZUmkRGfHSMdiL?authuser=1#scrollTo=EgNebQdkr7Wn
```{r}
# #load data
# library(data.table)
# 
# jpl <- list.files(path ="C:/Users/mirei/OneDrive/Documents/GitHub/ESS_224-CEE_260D/hw1/jpl_grace" ,pattern = "*.nc", full.names = T)
# 
# jpl_data <- lapply(jpl, nc_open) 
# 
# testjpl<- nc_open("C:/Users/mirei/OneDrive/Documents/GitHub/ESS_224-CEE_260D/hw1/jpl_grace/GRD-3_2002094-2002120_GRAC_JPLEM_BA01_0600_LND_v03.nc")
# # print(jpl_data_nc)
# print(testjpl)
# 
# #loading in GFZ data
# gfz <- list.files(path ="C:/Users/mirei/OneDrive/Documents/GitHub/ESS_224-CEE_260D/hw1/gfz_grace" ,pattern = "*.nc", full.names = T)
# 
# gfz_data <- lapply(gfz, nc_open) 
# 
# #load in CSR data
# csr <- list.files(path = "C:/Users/mirei/OneDrive/Documents/GitHub/ESS_224-CEE_260D/hw1/csr_grace", pattern = "*.nc", full.names = T)
# 
# csr_data <- lapply(csr, nc_open) 
# 
# csr_data_ras <- lapply(csr, raster::raster, lvar = 4, level = 1)
# csr_data_ras<-raster::brick(csr_data_ras) %>%
#   rotate()
# crs(csr_data_ras)
# 
# test_jpl <- raster("C:/Users/mirei/OneDrive/Documents/GitHub/ESS_224-CEE_260D/hw1/jpl_grace/GRD-3_2002094-2002120_GRAC_JPLEM_BA01_0600_LND_v03.nc") %>% rotate()
# 
# plot(test_jpl)


```


a.) (2 point) Extract the monthly change in water storage for each of the three GRACE models at the point -120.5, 37.5, a location near Hopeton, CA in the Central Valley (about midway between Modesto and Merced). Plot the individual solution estimates and the average of the three solutions on a single time series plot. Note that there are some gaps in the data record (explanation here: https://grace.jpl.nasa.gov/data/grace_months/). You’ll need to extract the day-of-year ranges from the filenames and convert them to dates to generate the time series.


#```{r}
# #csr_vals <-extract(csr_data_ras, matrix(c(-120.5, 37.5), ncol = 2))
# 
# test_val <- raster::extract(test_jpl, matrix(c(-120.5, 37.5), ncol = 2), fun=mean, 
#     df=TRUE)
# 
# # test_csr <- raster::raster(csr[1]) %>% rotate()
# 
# for (i in csr){
#   test_csr <- raster::raster(csr[i]) %>% rotate()
#   temp <- raster::extract(
#     test_csr,
#     matrix(c(-120.5, 37.5), ncol = 2),
#     fun=mean,
#     df=TRUE
#   )
# 
#   csr_vals <- csr_vals %>% rbind(temp)
# }





# # points <- matrix(c(-120.5, 37.5))
# point<- st_point(c(-120.5, 37.5)) %>% 
#   st_sfc()
# raster::beginCluster()
# csr_vals<- exact_extract(
#   csr_data_ras,
#   point
# )
# raster::endCluster()
# 
# raster:: beginCluster()
# vals<-terra:: extract(csr_data_ras, c(-120.5, 37.5))
# raster:: endCluster()
# 
# csr_df <- as.data.frame(csr_data_ras, xy = T)
#```

#```{r}
# test$var$lwe_thickness
# test$dim$time
# attributes(test$var)$names
# nc_open(csr[1])

# test1 <- nc_open("C:/Users/mirei/OneDrive/Documents/GitHub/ESS_224-CEE_260D/hw1/jpl_grace/GRD-3_2002094-2002120_GRAC_JPLEM_BA01_0600_LND_v03.nc")
# 
# # test1$dim$time$vals
# 
# #switch long coordinate to 360 range
# long_360 <- -120.5 %% 360
# 
# # Define our function
# process_nc <- function(files){
#   grace_vals <- c()
#     # iterate through the nc
#     for (i in 1:length(files)){
#         # open a connection to the ith nc file
#         nc_tmp <- nc_open(files[i])
#  
#         lon_index <- which(nc_tmp$dim$lon$vals == long_360)
#         lat_index <- which(nc_tmp$dim$lat$vals == 37.5)
#         #time_index <- nc_tmp$dim$time$vals
#         
#         nc_lwe_thickness <- ncvar_get(nc_tmp, attributes(nc_tmp$var)$names[1], start = c(lon_index, lat_index, 1))
#         
#         # store values from variables and atributes
#         
#         #append to list
#         grace_vals <- append(grace_vals, nc_lwe_thickness)
#         nc_close(nc_tmp)
#     }
#   vals_df <- as.data.frame(grace_vals)
#   return(vals_df)
# }
# 
# 
# data <- process_nc(csr)
# ```
# 
# 
# Resource used: https://www.r-bloggers.com/2016/08/a-netcdf-4-in-r-cheatsheet/
# 
# 
# 
# b.) (1 point) Interpret your plot in part ain the context of hydrological processes, climate, and water use in California. What trends do you observe? What might be causing the trends?
# 
# 
# Chunks that didn't work
# 
# #New method
# ```{r}
# cores <- detectCores() - 1
# 
# #will read in filename and explicitly open it as a NetCDF file
# 
# open_netcdf_as_rasterbrick <- function( ncdf_filename_input ) {
# 
#   ncdf_filename_input %>%
#   file.path("C:/Users/mirei/OneDrive/Documents/GitHub/ESS_224-CEE_260D/hw1/csr_grace", . ) %>%
#   nc_open( . )  %>%  # Open path as NetCDF file.
#   ncvar_get( . ) %>%  # Get NetCDF file.
# 
# 
#   # Transform NetCDF into raster brick.
# 
#   # NOTE: Your dimensions and CRS will differ,
#   # so these should be replaced.
# 
#   brick( . ,ymn = -0, ymx = 360, xmn = -90, xmx = 90,
#              crs = "+proj=longlat +datum=WGS84 +no_defs" ) %>%
#     rotate()
#   return( . )
# }
# ```
# 
# ```{r}
# #creating the bounding box to crop
# #-120.5, 37.5
# #note: latitude is first and then longitude
# #latitude is from north to south
# #longitude is from east to west 
# #when creating a bounding box in R, lat and long are switched in the arguments so long goes first and then lat
# # point<- st_point(c(-120.5, 37.5)) %>% 
# #   st_sfc()
# 
# coords <- matrix(c(
#   37.4, -120.4,
#   37.4, -120.6,
#   37.6, -120.6,
#   37.6, -120.4,
#   37.4, -120.4
# ),
# ncol = 2, byrow = TRUE
# )
# 
# P1 <- Polygon(coords)
# Ps1 <- SpatialPolygons(list(Polygons(list(P1), ID = "a")), proj4string = CRS("+proj=longlat +datum=WGS84 +no_defs"))
# ```
# 
# ```{r}
# # Small function 2) Transforms the raster brick to our shapefile.
# match_rainbrick_to_sp <- function( brick_input ) {
# 
#   # NOTE: Depending on your setting and the nature of the shapefile
#   # and NetCDF raster files you're using, you may have to do many more
#   # manipulations to make sure the raster layers align with the shapefile.
# 
#   # brick_input %>%
#   # # Reproject raster brick to the shapefile's coordinate system.
#   # projectRaster( . , crs = proj4string( countryshape ),
#   #                  method = "ngb" ) %>%
# 
#   # Crop to match the size of my country shapefile.
#   # raster::crop( . , extent(Ps1) ) %>%
#   # return( . )
#   raster::crop( brick_input , extent(Ps1) ) %>%
#   return( brick_input )
# }
```

#```{r}
# Small function 3) Extract data from a raster brick.
# generate_data_from_rasterandshape <- function( brick_input ) {
# 
# 	brick_input %>%
# 	# Take means according to the countryshape.
# 	# Make sure df = TRUE , so that output is a dataframe.
#     raster::extract( . , matrix(c(37.5, -120.5), ncol = 2) ,
#     				f=TRUE, fun = mean, na.rm = TRUE  )
#     return( . )
#   
# }
# 
# ```
# 
# ```{r}
# #define "big" function that extracts dataset from a NetCDF file
# generate_datatable_from_rasterbricks <- function( ncdf_filename_input ) {
# 
# 	# Note: the only argument is a NetCDF filename.
# 
# 	# Start with file argument and process with the sub-functions above.
# 	ncdf_filename_input %>%
# 
# 	open_netcdf_as_rasterbrick( . ) %>%
# 	match_rainbrick_to_sp( . ) %>%
# 	generate_data_from_rasterandshape( . ) -> sp_means_dataframe
# 
#   # return(sp_means_dataframe)
# 	# Go back to the file input name, create automatic names, and save.
# 	ncdf_filename_input %>%
# 
# 	# grab_year_from_inputfile( ) %>%
# 	write.csv( sp_means_dataframe ,
# 				file = file.path( . , "C:/Users/mirei/OneDrive/Desktop/ESS224/Pset1" ) )
# }
# ```
# 
# ```{r}
# #Set Up Environment to Run Big Function
# #generate list of NetCDF files
# raster_file_list <- list.files( path = "C:/Users/mirei/OneDrive/Documents/GitHub/ESS_224-CEE_260D/hw1/csr_grace" ,
# pattern = ".nc" , all.files = FALSE , full.names = FALSE )
# 
# # Run our big function on the list of NetCDF files.
# mclapply( raster_file_list , generate_datatable_from_rasterbricks )
# 
# # ---- 3. Assemble .CSV Files using Data.Table and Lapply.
# 
# # Fetch all files ending in .CSV in out output path.
# csv_file_list <- list.files( path = "C:/Users/mirei/OneDrive/Desktop/ESS224/Pset1" ,
#                              pattern = ".csv",
#                              all.files = FALSE,
#                              full.names = TRUE,
#                              recursive = FALSE )
# 
# # Take the list of saved files & "fast read" them into R.
# lapply( csv_file_list , fread , sep = "," ) %>%
# 
# # Transform the list of read files into a data.table:
# rbindlist( . ) -> big_datatable_csr
# ```

